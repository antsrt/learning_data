
[2025-04-11 11:40:04,334][root][INFO] - Converting mesh (8054339739791038996, -2719258046216355273) into convex hull.
[2025-04-11 11:40:07,931][root][INFO] - Converting mesh (-3902694956363739300, 5545577025147655822) into convex hull.
[2025-04-11 11:40:08,368][root][INFO] - Converting mesh (2807815759730869224, -4285963546846792471) into convex hull.
[2025-04-11 11:40:09,690][root][INFO] - Converting mesh (3653855925825839847, 4175084132693790403) into convex hull.
[2025-04-11 11:40:10,695][root][INFO] - Converting mesh (-6462015153952324784, -6207179737820557628) into convex hull.
Model horizon updated to 4.
Hallucination updates per training step updated to 505.
SAC buffer resized to 400000 samples.
Loading data
Creating stacked obs
Calculating rewards
Loading data
Creating stacked obs
Calculating rewards
Building transitions
/workspace/submodules/ssrl/ssrl/brax/training/replay_buffers.py:121: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  insert_size = jax.tree_flatten(samples)[0][0].shape[0] // shards
Rollout steps: 1002
Total reward / rollout steps: 0.29892067856786586
/usr/local/lib/python3.8/dist-packages/plotly/matplotlylib/renderer.py:645: UserWarning:
Looks like the annotation(s) you are trying
to draw lies/lay outside the given figure size.
Therefore, the resulting Plotly figure may not be
large enough to view the full text. To adjust
the size of the figure, use the 'width' and
'height' keys in the Layout object. Alternatively,
use the Margin object to adjust the figure's margins.
{'stats/avg_episode_length': Array(506., dtype=float64), 'stats/max_episode_length': Array(560., dtype=float64), 'stats/avg_forward_vel': Array(0.01932242, dtype=float64), 'stats/first_avg_forward_vel': Array(0.02035468, dtype=float64), 'stats/avg_side_vel': Array(0.0121183, dtype=float64), 'rollout_steps': 1002, 'stats/avg_reward_per_step': Array(0.29892068, dtype=float64), 'reward': Array(160.58109209, dtype=float64), 'total_reward': Array(299.51851993, dtype=float64), 'rew/penalty_torque_lim': Array(-0.09500191, dtype=float64), 'rew/rew_action': Array(0., dtype=float64), 'rew/rew_ang_change': Array(0.04895216, dtype=float64), 'rew/rew_ang_vel': Array(0., dtype=float64), 'rew/rew_cosmetic': Array(0., dtype=float64), 'rew/rew_energy': Array(0.00040983, dtype=float64), 'rew/rew_foot_z': Array(0., dtype=float64), 'rew/rew_forward_vel': Array(0.00831072, dtype=float64), 'rew/rew_joint_acc': Array(0., dtype=float64), 'rew/rew_joint_limits': Array(0., dtype=float64), 'rew/rew_pitch': Array(0.05018566, dtype=float64), 'rew/rew_roll': Array(0.05249786, dtype=float64), 'rew/rew_side_motion': Array(0.10580567, dtype=float64), 'rew/rew_torque_limits': Array(0., dtype=float64), 'rew/rew_turn': Array(0.0323351, dtype=float64), 'rew/rew_yaw': Array(0.06555832, dtype=float64), 'rew/rew_z_vel_change': Array(0.03225804, dtype=float64), 'stats/avg_turn_rate': Array(-0.02644138, dtype=float64)}
Starting training with the following parameters:
{'episode_length': 1000, 'policy_repeat': 1, 'num_epochs': 30, 'model_trains_per_epoch': 1, 'training_steps_per_model_train': 1, 'env_steps_per_training_step': 1000, 'model_rollouts_per_hallucination_update': 400, 'sac_grad_updates_per_hallucination_update': 40, 'init_exploration_steps': 1000, 'clear_model_buffer_after_model_train': False, 'action_repeat': 1, 'obs_history_length': 5, 'num_envs': 1, 'num_evals': 31, 'num_eval_envs': 1, 'policy_normalize_observations': False, 'model_learning_rate': 0.001, 'model_training_batch_size': 200, 'model_training_max_sgd_steps_per_epoch': 20, 'model_training_max_epochs': 1000, 'model_training_convergence_criteria': 0.01, 'model_training_consec_converged_epochs': 6, 'model_training_abs_criteria': None, 'model_training_test_ratio': 0.2, 'model_training_weight_decay': True, 'model_training_stop_gradient': False, 'model_loss_horizon': 4, 'model_check_done_condition': True, 'max_env_buffer_size': 30000, 'max_model_buffer_size': 400000, 'sac_learning_rate': 0.0002, 'sac_discounting': 0.99, 'sac_batch_size': 256, 'real_ratio': 0.06, 'sac_reward_scaling': 1.0, 'sac_tau': 0.001, 'sac_fixed_alpha': None, 'seed': 0, 'deterministic_in_env': True, 'deterministic_eval': True, 'hallucination_max_std': -1.0, 'zero_final_layer_of_policy': False}
Training...
Model epoch 0: train total loss -22.584347160476074, train mean loss 0.25381719562703636, test mean loss [0.27797071 0.22725398 0.24071393 0.25689617 0.27678893 0.17104483
 0.23534942]
Model epoch 1: train total loss -25.611959154192295, train mean loss 0.19723174402821267, test mean loss [0.23573781 0.17926079 0.19227972 0.21371647 0.20609221 0.13845613
 0.1726146 ]
Model epoch 2: train total loss -27.434739425234604, train mean loss 0.16356526297350632, test mean loss [0.20910799 0.1437256  0.16070885 0.16977706 0.15203408 0.12469718
 0.14561776]
Model epoch 3: train total loss -28.66183742722572, train mean loss 0.1376512388331351, test mean loss [0.17983334 0.12413778 0.13633188 0.13815836 0.13269359 0.11198038
 0.13010069]
Model epoch 4: train total loss -29.402057714615104, train mean loss 0.12842751011064402, test mean loss [0.15415059 0.11244217 0.11954869 0.12032223 0.11744226 0.10413151
 0.11680386]
Model epoch 5: train total loss -30.268112041328312, train mean loss 0.10913869606941681, test mean loss [0.13385644 0.10306707 0.10847636 0.10814778 0.10921089 0.09747162
 0.10688117]
Model epoch 6: train total loss -30.86208747947612, train mean loss 0.10194294028440823, test mean loss [0.11764688 0.099431   0.10046969 0.09895864 0.10072047 0.09514629
 0.09812199]
Model epoch 7: train total loss -31.35997645355363, train mean loss 0.09334074280128002, test mean loss [0.10913104 0.09399358 0.0946817  0.0939075  0.09321521 0.09220735
 0.09199107]
Model epoch 8: train total loss -31.6870504444449, train mean loss 0.09268394764004308, test mean loss [0.10172934 0.08936464 0.0907232  0.09074106 0.08956682 0.08766387
 0.0884426 ]
Model epoch 9: train total loss -32.130516695563124, train mean loss 0.08651825379930601, test mean loss [0.09459416 0.08747291 0.08784975 0.0867281  0.08672064 0.08629047
 0.08643007]
Model epoch 10: train total loss -32.409312678847726, train mean loss 0.08439512445559297, test mean loss [0.09210223 0.08284942 0.08410699 0.08482376 0.08383239 0.08422343
 0.08239348]
Model epoch 11: train total loss -32.539000950258675, train mean loss 0.08438658218488475, test mean loss [0.08782243 0.08184878 0.08177351 0.08122698 0.08150761 0.08041145
 0.08018926]
Model epoch 12: train total loss -32.98729805904535, train mean loss 0.08221352503614578, test mean loss [0.08581643 0.07922023 0.08104591 0.08168041 0.08026604 0.08087658
 0.07857334]
Model epoch 13: train total loss -33.22488118031476, train mean loss 0.07968257075585228, test mean loss [0.08448585 0.07708589 0.08093543 0.07993571 0.07754131 0.07877347
 0.074984  ]
Model epoch 14: train total loss -33.4109352320076, train mean loss 0.07755546720765392, test mean loss [0.08234575 0.07556176 0.07888222 0.07980343 0.0765205  0.07854263
 0.07392844]
Model epoch 15: train total loss -33.51826095693299, train mean loss 0.0770053869506432, test mean loss [0.07966441 0.07361855 0.07750537 0.07819676 0.07989609 0.0782372
 0.0739107 ]
Model epoch 16: train total loss -33.79733729654019, train mean loss 0.07562672050065476, test mean loss [0.07967602 0.07440888 0.07782327 0.07682754 0.0745308  0.0775261
 0.07135782]
Model epoch 17: train total loss -33.83224286576102, train mean loss 0.07716107942417844, test mean loss [0.07978634 0.07167694 0.07774455 0.07467278 0.07549276 0.07629625
 0.07052272]
Model epoch 18: train total loss -34.00505507795543, train mean loss 0.07691739447283324, test mean loss [0.08058746 0.07305363 0.08023617 0.07808566 0.07339954 0.07582503
 0.07152395]
Model epoch 19: train total loss -34.07755686706942, train mean loss 0.07535427174430066, test mean loss [0.07453487 0.070407   0.07775317 0.07481166 0.07271722 0.07426364
 0.0717386 ]
Model epoch 20: train total loss -34.188531875355196, train mean loss 0.07428544871721433, test mean loss [0.07845399 0.07123726 0.07535731 0.07152418 0.07643105 0.07620457
 0.06929753]
Model epoch 21: train total loss -34.35053214280947, train mean loss 0.0719724479933963, test mean loss [0.07726913 0.07004963 0.07398454 0.07573805 0.07414982 0.07463838
 0.07050186]
Model epoch 22: train total loss -34.397420664446315, train mean loss 0.07394208586299107, test mean loss [0.07737817 0.06984808 0.07731989 0.07263004 0.07276418 0.07711116
 0.07273203]
Model epoch 23: train total loss -34.50435124723731, train mean loss 0.07466437369119673, test mean loss [0.07704863 0.06868103 0.07689829 0.07514682 0.07592229 0.07621326
 0.07165586]
Model epoch 24: train total loss -34.603855924876065, train mean loss 0.07161030012896706, test mean loss [0.07641237 0.07091545 0.07466186 0.07265647 0.073495   0.0738622
 0.06994428]
Model epoch 25: train total loss -34.67325838870733, train mean loss 0.07205427697672927, test mean loss [0.07632036 0.06953572 0.07595263 0.07329169 0.07457479 0.07198435
 0.07069991]
Model epoch 26: train total loss -34.67920474361498, train mean loss 0.07068349241041497, test mean loss [0.07457988 0.06633102 0.07523094 0.0708449  0.0741082  0.07191539
 0.0720015 ]
Model epoch 27: train total loss -34.73969929573836, train mean loss 0.07246467890525744, test mean loss [0.07729694 0.07049593 0.07792965 0.07051096 0.07255961 0.07504849
 0.07012353]
Model epoch 28: train total loss -34.87227335685138, train mean loss 0.07171406009909294, test mean loss [0.07653058 0.06817508 0.07476433 0.07227684 0.07476171 0.07369696
 0.06699442]
Model epoch 29: train total loss -35.050399194055956, train mean loss 0.07240270915340746, test mean loss [0.07724991 0.0708575  0.07422838 0.07110442 0.07311601 0.07264689
 0.07133224]
Model epoch 30: train total loss -35.14180966226006, train mean loss 0.07245418253633652, test mean loss [0.07232074 0.06930391 0.07481832 0.0739898  0.07252537 0.07291544
 0.07152165]
Model epoch 31: train total loss -35.17702924097655, train mean loss 0.07180364213736339, test mean loss [0.07229143 0.07167914 0.0738228  0.07453957 0.0710993  0.07241497
 0.06929766]
Model epoch 32: train total loss -35.213001254273614, train mean loss 0.0729106083650549, test mean loss [0.07648707 0.06849889 0.07553707 0.07273432 0.0715736  0.07127701
 0.06878639]
Model epoch 33: train total loss -35.26992947870913, train mean loss 0.07081568452787196, test mean loss [0.07272271 0.06574681 0.07445706 0.07455471 0.06870576 0.07367059
 0.07075613]
Model epoch 34: train total loss -35.470034637730954, train mean loss 0.07138094503953797, test mean loss [0.07688766 0.07006461 0.07335191 0.07054698 0.07057523 0.07125247
 0.07172466]
Model epoch 35: train total loss -35.531104901830986, train mean loss 0.07049445569578738, test mean loss [0.07832219 0.07025771 0.07351904 0.06948992 0.0691477  0.07126519
 0.07417633]
Model epoch 36: train total loss -35.53395958332742, train mean loss 0.07283342019121307, test mean loss [0.07609702 0.07069004 0.07262294 0.07230971 0.07382459 0.07174489
 0.06882515]
Model epoch 37: train total loss -35.600579260985356, train mean loss 0.07233724893740773, test mean loss [0.07357507 0.07057185 0.06989788 0.07550516 0.07019439 0.071879
 0.07141433]
Model epoch 38: train total loss -35.77760103742768, train mean loss 0.06974700386109862, test mean loss [0.07669032 0.06730809 0.07312525 0.07522287 0.06613309 0.07100403
 0.06724353]
Model epoch 39: train total loss -35.77993821399678, train mean loss 0.07067219070745429, test mean loss [0.07733204 0.07049192 0.07101332 0.07309465 0.06958923 0.06969142
 0.06731431]
Model epoch 40: train total loss -35.7157031714189, train mean loss 0.07218973551366444, test mean loss [0.07334288 0.06561089 0.07122507 0.0725578  0.06988934 0.07450189
 0.07025386]
Model epoch 41: train total loss -35.855554440853005, train mean loss 0.06930715434757098, test mean loss [0.07399082 0.06835835 0.06856721 0.07204032 0.07147477 0.06776748
 0.06783354]
Model epoch 42: train total loss -35.89163303447539, train mean loss 0.07009159354647955, test mean loss [0.07806404 0.06498928 0.07067899 0.07069383 0.07085388 0.07281178
 0.06940058]
Model epoch 43: train total loss -35.92856236974473, train mean loss 0.07132269970096411, test mean loss [0.07506236 0.06996543 0.07142076 0.07302532 0.0715578  0.07257017
 0.07321703]
Model epoch 44: train total loss -35.926097088025806, train mean loss 0.07126913438575204, test mean loss [0.07472679 0.06660857 0.07225116 0.07245574 0.06949149 0.07311411
 0.07010514]
Model epoch 45: train total loss -36.04109144547448, train mean loss 0.07036873292789891, test mean loss [0.07495796 0.06738297 0.06929177 0.07103221 0.07365588 0.06970142
 0.0701204 ]
Model epoch 46: train total loss -36.143402332093636, train mean loss 0.06931122762466181, test mean loss [0.07397014 0.07117956 0.06874127 0.07529584 0.07290399 0.06953693
 0.06732929]
Model epoch 47: train total loss -36.258325447009454, train mean loss 0.06988124399665223, test mean loss [0.07124353 0.06903209 0.07097654 0.07014721 0.07037087 0.06994796
 0.07160121]
Model epoch 48: train total loss -36.191639713686605, train mean loss 0.06984485442964856, test mean loss [0.07193521 0.06728506 0.07286757 0.07170838 0.0686879  0.07146432
 0.06905897]
Model epoch 49: train total loss -36.341895909216866, train mean loss 0.06854290272739455, test mean loss [0.07148604 0.06536831 0.07150637 0.07036631 0.07254199 0.07253549
 0.07311001]
Model epoch 50: train total loss -36.46440018790482, train mean loss 0.068211706864308, test mean loss [0.07439602 0.06378648 0.06786907 0.07189663 0.06811133 0.07002585
 0.06774274]
Model epoch 51: train total loss -36.42417431801973, train mean loss 0.06901199354293747, test mean loss [0.07326773 0.06644842 0.07130611 0.07442248 0.06873226 0.07506564
 0.06787263]
Model epoch 52: train total loss -36.59203238652507, train mean loss 0.07072371609945709, test mean loss [0.07218837 0.06977371 0.07046467 0.07501439 0.0715465  0.07240518
 0.06951465]
Model epoch 53: train total loss -36.61151069122475, train mean loss 0.06874128808577945, test mean loss [0.07284515 0.06947469 0.07044778 0.07193255 0.07104654 0.07255591
 0.06745234]
Model epoch 54: train total loss -36.62158318851655, train mean loss 0.06961240882919877, test mean loss [0.07295399 0.06671436 0.07160677 0.07174525 0.07126056 0.06946002
 0.06910302]
Model epoch 55: train total loss -36.611261548309535, train mean loss 0.06975318921361136, test mean loss [0.07309756 0.06840988 0.06978897 0.07619583 0.06930543 0.06883953
 0.07151751]
Model epoch 56: train total loss -36.654754392093814, train mean loss 0.06900696995911883, test mean loss [0.07177091 0.06833468 0.07022542 0.07045327 0.06859376 0.07216841
 0.06930582]
Model trained in 57 epochs with 3004 transitions.
/workspace/submodules/ssrl/ssrl/brax/training/replay_buffers.py:121: FutureWarning:
jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
{'steps': 3004, 'epoch': 2, 'training/model_train_time': 172.58564400672913, 'training/other_time': 162.47624039649963, 'training/model_horizon': 4, 'training/hallucination_updates_per_training_step': 505, 'training/env_buffer_size': Array(3004, dtype=int32), 'training/reset_critic': 0, 'model/train_total_loss': Array(-36.65475439, dtype=float64, weak_type=True), 'model/train_mean_loss': Array(0.06900697, dtype=float64), 'model/test_total_loss': Array(-36.22939566, dtype=float64), 'model/test_mean_loss': Array(0.07012175, dtype=float64), 'model/train_epochs': 57, 'model/sec_per_epoch': 2.962966387731987, 'sac/actor_loss': Array(11.10259098, dtype=float64), 'sac/alpha': Array(0.03730894, dtype=float32), 'sac/alpha_loss': Array(0.01353122, dtype=float64), 'sac/buffer_current_size': Array(377046.03, dtype=float32), 'sac/critic_loss': Array(0.97498681, dtype=float64), 'rollout/stats/avg_episode_length': Array(506., dtype=float64), 'rollout/stats/max_episode_length': Array(560., dtype=float64), 'rollout/stats/avg_forward_vel': Array(0.01932242, dtype=float64), 'rollout/stats/first_avg_forward_vel': Array(0.02035468, dtype=float64), 'rollout/stats/avg_side_vel': Array(0.0121183, dtype=float64), 'rollout/rollout_steps': 1002, 'rollout/stats/avg_reward_per_step': Array(0.29892068, dtype=float64), 'rollout/reward': Array(160.58109209, dtype=float64), 'rollout/total_reward': Array(299.51851993, dtype=float64), 'rollout/rew/penalty_torque_lim': Array(-0.09500191, dtype=float64), 'rollout/rew/rew_action': Array(0., dtype=float64), 'rollout/rew/rew_ang_change': Array(0.04895216, dtype=float64), 'rollout/rew/rew_ang_vel': Array(0., dtype=float64), 'rollout/rew/rew_cosmetic': Array(0., dtype=float64), 'rollout/rew/rew_energy': Array(0.00040983, dtype=float64), 'rollout/rew/rew_foot_z': Array(0., dtype=float64), 'rollout/rew/rew_forward_vel': Array(0.00831072, dtype=float64), 'rollout/rew/rew_joint_acc': Array(0., dtype=float64), 'rollout/rew/rew_joint_limits': Array(0., dtype=float64), 'rollout/rew/rew_pitch': Array(0.05018566, dtype=float64), 'rollout/rew/rew_roll': Array(0.05249786, dtype=float64), 'rollout/rew/rew_side_motion': Array(0.10580567, dtype=float64), 'rollout/rew/rew_torque_limits': Array(0., dtype=float64), 'rollout/rew/rew_turn': Array(0.0323351, dtype=float64), 'rollout/rew/rew_yaw': Array(0.06555832, dtype=float64), 'rollout/rew/rew_z_vel_change': Array(0.03225804, dtype=float64), 'rollout/stats/avg_turn_rate': Array(-0.02644138, dtype=float64)}