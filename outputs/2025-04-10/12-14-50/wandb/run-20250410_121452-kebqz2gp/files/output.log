
[2025-04-10 12:14:53,353][root][INFO] - Converting mesh (-3304419525266437758, 1375764831547189877) into convex hull.
[2025-04-10 12:14:56,895][root][INFO] - Converting mesh (-8287652733727608972, 6227948526883712282) into convex hull.
[2025-04-10 12:14:57,325][root][INFO] - Converting mesh (7424039713530393563, -2849888256372100436) into convex hull.
[2025-04-10 12:14:58,623][root][INFO] - Converting mesh (8586653855090292140, 5343805049073217975) into convex hull.
[2025-04-10 12:14:59,613][root][INFO] - Converting mesh (-3084412802585681308, -2687030666440043369) into convex hull.
Loading data
Creating stacked obs
Calculating rewards
Building transitions
Rollout steps: 1000
Total reward / rollout steps: 0.2524857479846655
/workspace/submodules/ssrl/ssrl/brax/training/replay_buffers.py:121: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
  insert_size = jax.tree_flatten(samples)[0][0].shape[0] // shards
/usr/local/lib/python3.8/dist-packages/plotly/matplotlylib/renderer.py:645: UserWarning:
Looks like the annotation(s) you are trying
to draw lies/lay outside the given figure size.
Therefore, the resulting Plotly figure may not be
large enough to view the full text. To adjust
the size of the figure, use the 'width' and
'height' keys in the Layout object. Alternatively,
use the Margin object to adjust the figure's margins.
{'stats/avg_episode_length': Array(1005., dtype=float64), 'stats/max_episode_length': Array(1005., dtype=float64), 'stats/avg_forward_vel': Array(0.01364337, dtype=float64), 'stats/first_avg_forward_vel': Array(0.01364337, dtype=float64), 'stats/avg_side_vel': Array(-0.02647018, dtype=float64), 'rollout_steps': 1000, 'stats/avg_reward_per_step': Array(0.25248575, dtype=float64), 'reward': Array(252.48574798, dtype=float64), 'total_reward': Array(252.48574798, dtype=float64), 'rew/penalty_torque_lim': Array(-0.09928996, dtype=float64), 'rew/rew_action': Array(0., dtype=float64), 'rew/rew_ang_change': Array(0.05124828, dtype=float64), 'rew/rew_ang_vel': Array(0., dtype=float64), 'rew/rew_cosmetic': Array(0., dtype=float64), 'rew/rew_energy': Array(0.00057291, dtype=float64), 'rew/rew_foot_z': Array(0., dtype=float64), 'rew/rew_forward_vel': Array(0.00586812, dtype=float64), 'rew/rew_joint_acc': Array(0., dtype=float64), 'rew/rew_joint_limits': Array(0., dtype=float64), 'rew/rew_pitch': Array(0.04978685, dtype=float64), 'rew/rew_roll': Array(0.0535181, dtype=float64), 'rew/rew_side_motion': Array(0.10024919, dtype=float64), 'rew/rew_torque_limits': Array(0., dtype=float64), 'rew/rew_turn': Array(0.03633915, dtype=float64), 'rew/rew_yaw': Array(0.02344678, dtype=float64), 'rew/rew_z_vel_change': Array(0.03225805, dtype=float64), 'stats/avg_turn_rate': Array(-0.11795346, dtype=float64)}
Starting training with the following parameters:
{'episode_length': 1000, 'policy_repeat': 1, 'num_epochs': 30, 'model_trains_per_epoch': 1, 'training_steps_per_model_train': 1, 'env_steps_per_training_step': 1000, 'model_rollouts_per_hallucination_update': 400, 'sac_grad_updates_per_hallucination_update': 40, 'init_exploration_steps': 1000, 'clear_model_buffer_after_model_train': False, 'action_repeat': 1, 'obs_history_length': 5, 'num_envs': 1, 'num_evals': 31, 'num_eval_envs': 1, 'policy_normalize_observations': False, 'model_learning_rate': 0.001, 'model_training_batch_size': 200, 'model_training_max_sgd_steps_per_epoch': 20, 'model_training_max_epochs': 1000, 'model_training_convergence_criteria': 0.01, 'model_training_consec_converged_epochs': 6, 'model_training_abs_criteria': None, 'model_training_test_ratio': 0.2, 'model_training_weight_decay': True, 'model_training_stop_gradient': False, 'model_loss_horizon': 4, 'model_check_done_condition': True, 'max_env_buffer_size': 30000, 'max_model_buffer_size': 400000, 'sac_learning_rate': 0.0002, 'sac_discounting': 0.99, 'sac_batch_size': 256, 'real_ratio': 0.06, 'sac_reward_scaling': 1.0, 'sac_tau': 0.001, 'sac_fixed_alpha': None, 'seed': 0, 'deterministic_in_env': True, 'deterministic_eval': True, 'hallucination_max_std': -1.0, 'zero_final_layer_of_policy': False}
Training...
Model epoch 0: train total loss -2.3704454592057234, train mean loss 0.3907770168864446, test mean loss [0.40480692 0.40652053 0.40486021 0.40632784 0.40690751 0.40779772
 0.40002628]
Model epoch 1: train total loss -7.085891609749733, train mean loss 0.3805834915915105, test mean loss [0.39169821 0.40157041 0.39429655 0.39711961 0.40287242 0.40821039
 0.40433712]
Model epoch 2: train total loss -11.172629202297745, train mean loss 0.37264688118503275, test mean loss [0.38197078 0.39321507 0.38432722 0.38607086 0.39685358 0.40015587
 0.39595646]
Model epoch 3: train total loss -13.786574642995134, train mean loss 0.36263093055420265, test mean loss [0.36412587 0.38317084 0.3667726  0.37173683 0.38407399 0.39044251
 0.38655103]
Model epoch 4: train total loss -16.55229860321324, train mean loss 0.34564234461270815, test mean loss [0.34063404 0.36725237 0.34452476 0.34918539 0.36389322 0.3749671
 0.371382  ]
Model epoch 5: train total loss -19.35364384854665, train mean loss 0.32543876604038874, test mean loss [0.32222672 0.34772621 0.32278589 0.3300903  0.34485572 0.35784402
 0.35487342]
Model epoch 6: train total loss -21.756786373995478, train mean loss 0.310224759530858, test mean loss [0.30680235 0.33009178 0.3060549  0.31683919 0.3288976  0.34223988
 0.34061779]
Model epoch 7: train total loss -23.292620155918026, train mean loss 0.3001674297838976, test mean loss [0.29288692 0.31711039 0.29826332 0.3064993  0.31463659 0.32957167
 0.32879534]
Model epoch 8: train total loss -23.831786077087965, train mean loss 0.2889507097316472, test mean loss [0.28970586 0.30221765 0.29247602 0.30467649 0.31120896 0.31750033
 0.32014252]
Model epoch 9: train total loss -24.163274267959814, train mean loss 0.28316616779979914, test mean loss [0.29226215 0.29656788 0.28835524 0.29421476 0.30121975 0.31233749
 0.31219559]
Model epoch 10: train total loss -24.487230520880964, train mean loss 0.2791779932540906, test mean loss [0.28668877 0.29115461 0.28750365 0.28861464 0.29486148 0.30316676
 0.30426413]
Model epoch 11: train total loss -24.79651770668887, train mean loss 0.2711609307374401, test mean loss [0.28328064 0.28718308 0.28522438 0.28380578 0.28853319 0.29646654
 0.29763716]
Model epoch 12: train total loss -25.03581055929228, train mean loss 0.2682555122284822, test mean loss [0.28017617 0.28535594 0.28144039 0.28314547 0.28638301 0.29043385
 0.29201086]
Model epoch 13: train total loss -25.290460260678376, train mean loss 0.2662845990107899, test mean loss [0.27732476 0.28342942 0.27821614 0.2805325  0.28472791 0.286828
 0.28607596]
Model epoch 14: train total loss -25.596496889467435, train mean loss 0.2637038721319423, test mean loss [0.27285726 0.27995496 0.27391647 0.27644113 0.28185726 0.28414702
 0.28360464]
Model epoch 15: train total loss -25.8220563607774, train mean loss 0.26453609253995314, test mean loss [0.26911676 0.27657366 0.27364898 0.27182041 0.2798613  0.28172373
 0.27991826]
Model epoch 16: train total loss -26.211597162664432, train mean loss 0.2591692514047319, test mean loss [0.26900552 0.27313788 0.26868889 0.2683983  0.2755671  0.27907705
 0.2766331 ]
Model epoch 17: train total loss -26.682150726301188, train mean loss 0.25346810820866034, test mean loss [0.26427943 0.27066653 0.26538482 0.26278226 0.2723949  0.27551495
 0.27141906]
Model epoch 18: train total loss -27.159244506278636, train mean loss 0.2535537476107954, test mean loss [0.25887212 0.26790253 0.26147945 0.25947857 0.2688127  0.27313746
 0.26790472]
Model epoch 19: train total loss -27.652992944314445, train mean loss 0.24788298494053762, test mean loss [0.25283815 0.26411945 0.25788484 0.25801409 0.26591894 0.27189808
 0.26295839]
Model epoch 20: train total loss -28.346170782654745, train mean loss 0.24635614718707957, test mean loss [0.24792994 0.26037496 0.2534234  0.25537492 0.26301681 0.26760216
 0.26023148]
Model epoch 21: train total loss -28.987321958574093, train mean loss 0.2413481180795161, test mean loss [0.24424996 0.25613414 0.24710955 0.25055456 0.26010269 0.26650122
 0.25607155]
Model epoch 22: train total loss -29.612104479391434, train mean loss 0.23669915415794054, test mean loss [0.24080733 0.25356038 0.2438455  0.24576068 0.25741474 0.26373119
 0.25292095]
Model epoch 23: train total loss -29.997109430150005, train mean loss 0.23581397622906486, test mean loss [0.23625153 0.24968437 0.23797409 0.2390018  0.2550324  0.26060199
 0.24961628]
Model epoch 24: train total loss -30.527870188196182, train mean loss 0.2319115651371256, test mean loss [0.22922378 0.2452451  0.23364724 0.23324124 0.25183076 0.25628291
 0.24543508]
Model epoch 25: train total loss -30.993975825063576, train mean loss 0.224376988969562, test mean loss [0.22020266 0.23938436 0.22980399 0.22796456 0.24678005 0.25064822
 0.24038536]
Model epoch 26: train total loss -31.438841176799677, train mean loss 0.21768633565113754, test mean loss [0.21289073 0.23227813 0.22178931 0.22293544 0.23976689 0.2446628
 0.23598881]
Model epoch 27: train total loss -31.917706479854022, train mean loss 0.2106932738899406, test mean loss [0.2051497  0.22618957 0.21464759 0.21483357 0.23136084 0.23903062
 0.22986749]
Model epoch 28: train total loss -32.34631952833465, train mean loss 0.2048918941995306, test mean loss [0.19672719 0.21940097 0.20898976 0.20733589 0.2251821  0.23255557
 0.22482721]
Model epoch 29: train total loss -32.454026178871935, train mean loss 0.20017377164144223, test mean loss [0.18857105 0.21171954 0.20198959 0.20094291 0.21896031 0.22554892
 0.22006906]
Model epoch 30: train total loss -32.90739396951913, train mean loss 0.1917428982205215, test mean loss [0.18413292 0.20417576 0.19573601 0.19345611 0.21126976 0.21884686
 0.21505955]
Model epoch 31: train total loss -33.23250395435402, train mean loss 0.18344585139790215, test mean loss [0.17663166 0.19803222 0.1890976  0.18647943 0.20378048 0.21190406
 0.20643152]
Model epoch 32: train total loss -33.54433467833197, train mean loss 0.17719358230343882, test mean loss [0.16934319 0.19042544 0.18099715 0.18162392 0.19921215 0.20498563
 0.20048827]
Model epoch 33: train total loss -33.733140117704146, train mean loss 0.1717905286537003, test mean loss [0.16072377 0.18378963 0.17496961 0.173146   0.19172151 0.19537173
 0.19345947]
Model epoch 34: train total loss -34.10155842763847, train mean loss 0.1655123597497912, test mean loss [0.15532987 0.1764579  0.16620039 0.16746322 0.18607981 0.18865867
 0.18711648]
Model epoch 35: train total loss -34.29637055832948, train mean loss 0.15830872706881496, test mean loss [0.14618766 0.16791801 0.16054905 0.15997092 0.18066279 0.18008202
 0.17979597]
Model epoch 36: train total loss -34.602909688830714, train mean loss 0.15032586223154779, test mean loss [0.14062377 0.16034051 0.15233276 0.15402172 0.17380751 0.17157352
 0.1695881 ]
Model epoch 37: train total loss -34.81177185780843, train mean loss 0.143631317517953, test mean loss [0.13283304 0.15446773 0.14558094 0.14586682 0.1703711  0.16530727
 0.16172229]
Model epoch 38: train total loss -35.02305485388967, train mean loss 0.1370665828921801, test mean loss [0.12525001 0.14523294 0.13653971 0.13942124 0.16339739 0.15629365
 0.1529047 ]
Model epoch 39: train total loss -35.31165736742465, train mean loss 0.12902740945089866, test mean loss [0.11759379 0.13812545 0.13044173 0.13301658 0.15667568 0.14895371
 0.14623276]
Model epoch 40: train total loss -35.4963569233968, train mean loss 0.12431481930469149, test mean loss [0.11067496 0.13152627 0.12253734 0.12742158 0.15155294 0.14052021
 0.13795727]
Model epoch 41: train total loss -35.80645562005707, train mean loss 0.11469801586349487, test mean loss [0.1031085  0.12245066 0.11329197 0.11855207 0.14317913 0.13355214
 0.12829259]
Model epoch 42: train total loss -35.93985356483959, train mean loss 0.10855366836677155, test mean loss [0.09676167 0.11426006 0.10996296 0.11235081 0.13719087 0.12511377
 0.12173955]
Model epoch 43: train total loss -36.18731851217681, train mean loss 0.10212668516192, test mean loss [0.09056302 0.10695833 0.10128223 0.1060639  0.13122813 0.11589037
 0.11435907]
Model epoch 44: train total loss -36.38629317589898, train mean loss 0.09437872643877773, test mean loss [0.08524046 0.09939031 0.09549126 0.09938862 0.12238898 0.10740967
 0.10731368]
Model epoch 45: train total loss -36.54649345545646, train mean loss 0.08771327436058947, test mean loss [0.07929155 0.09281044 0.0918599  0.0919688  0.11608699 0.09891433
 0.1000091 ]
Model epoch 46: train total loss -36.66796324633511, train mean loss 0.0828139502434807, test mean loss [0.07448132 0.08479218 0.08516798 0.0869501  0.10718049 0.09097477
 0.09193   ]
Model epoch 47: train total loss -36.84096825135484, train mean loss 0.07808678049720759, test mean loss [0.0690341  0.07938471 0.07979168 0.08131252 0.10103893 0.08375455
 0.08805015]
Model epoch 48: train total loss -37.069086421596275, train mean loss 0.07131638627166055, test mean loss [0.06570256 0.07298125 0.07647239 0.07859921 0.09325175 0.07725299
 0.08135046]
Model epoch 49: train total loss -37.09839632284538, train mean loss 0.0692977766742884, test mean loss [0.06491437 0.06838892 0.07214867 0.07322935 0.08768164 0.07315857
 0.07693773]
Model epoch 50: train total loss -37.34065157680551, train mean loss 0.0650634087545361, test mean loss [0.06132867 0.06553734 0.06856327 0.06962492 0.0835075  0.06926767
 0.07358033]
Model epoch 51: train total loss -37.45988310942366, train mean loss 0.06090249068230903, test mean loss [0.05955032 0.0617611  0.06677914 0.06667832 0.07789887 0.06342999
 0.06804459]
Model epoch 52: train total loss -37.58429815324294, train mean loss 0.05873759489060099, test mean loss [0.05779421 0.05945714 0.06353219 0.06459667 0.07447785 0.06245565
 0.06484329]
Model epoch 53: train total loss -37.785988849175574, train mean loss 0.05534874856096778, test mean loss [0.05524841 0.05793669 0.06032981 0.06076581 0.06872406 0.05926684
 0.06218529]
Model epoch 54: train total loss -37.80397511268359, train mean loss 0.05468698837646583, test mean loss [0.05366221 0.05492293 0.05834716 0.05921354 0.06634168 0.05783801
 0.05934102]
Model epoch 55: train total loss -38.00906077122809, train mean loss 0.05227971240703836, test mean loss [0.05282459 0.05394697 0.05922516 0.05724815 0.06351344 0.05531499
 0.05708953]
Model epoch 56: train total loss -38.13470354192397, train mean loss 0.050643431138000516, test mean loss [0.05148665 0.05356673 0.05517878 0.05672824 0.06012395 0.05363737
 0.05612301]
Model epoch 57: train total loss -38.22905972282931, train mean loss 0.049715141026922816, test mean loss [0.05088602 0.05181335 0.05322064 0.05490584 0.05893224 0.05301874
 0.05408142]
Model epoch 58: train total loss -38.35750282299453, train mean loss 0.04827152735729315, test mean loss [0.05017762 0.04971839 0.05441067 0.05254125 0.05784542 0.05127906
 0.0532311 ]
Model epoch 59: train total loss -38.38308363787762, train mean loss 0.04816312322079788, test mean loss [0.04994116 0.05023311 0.05273925 0.05348694 0.05553253 0.05054741
 0.05138939]
Model epoch 60: train total loss -38.43159271559421, train mean loss 0.04713840209847758, test mean loss [0.05097805 0.04949789 0.05143933 0.05271424 0.05487166 0.04973887
 0.05068623]
Model epoch 61: train total loss -38.54362786089514, train mean loss 0.04617730408616535, test mean loss [0.04865095 0.04773915 0.05275881 0.04979499 0.05359236 0.050033
 0.04891895]
Model epoch 62: train total loss -38.605750246211414, train mean loss 0.04642761122294815, test mean loss [0.04852352 0.04883207 0.04898506 0.04997119 0.05323262 0.04924551
 0.0506696 ]
Model epoch 63: train total loss -38.70853619894109, train mean loss 0.04516575114993505, test mean loss [0.04740805 0.04842771 0.05121374 0.04903113 0.05118026 0.04826187
 0.04785768]
Model epoch 64: train total loss -38.71861170191389, train mean loss 0.0453022504794323, test mean loss [0.04787777 0.04637581 0.04894832 0.05099193 0.050852   0.04820285
 0.04736156]
Model epoch 65: train total loss -38.80562946187302, train mean loss 0.0440177413694862, test mean loss [0.0470482  0.04836777 0.04777582 0.04779998 0.04937103 0.04741441
 0.04775329]
Model epoch 66: train total loss -38.8713670937439, train mean loss 0.04425735561372148, test mean loss [0.04829131 0.04752131 0.04727872 0.04814388 0.04915117 0.04763924
 0.04666298]
Model epoch 67: train total loss -39.02059078813067, train mean loss 0.04378148086904344, test mean loss [0.04623406 0.0464245  0.04683019 0.04973059 0.05103234 0.04749437
 0.04681216]
Model epoch 68: train total loss -39.0605040494219, train mean loss 0.043144181862797476, test mean loss [0.04613491 0.04719782 0.04875347 0.04741173 0.04800331 0.04658008
 0.04573416]
Model epoch 69: train total loss -39.00861412788495, train mean loss 0.04341071115920015, test mean loss [0.0451494  0.04752999 0.04722522 0.0477628  0.04919122 0.04606597
 0.04510938]
Model epoch 70: train total loss -39.10783427967005, train mean loss 0.04300529307685525, test mean loss [0.04606599 0.04548722 0.04756486 0.04702721 0.04753514 0.04771248
 0.04499634]
Model epoch 71: train total loss -39.2010886887394, train mean loss 0.04224554381432868, test mean loss [0.04545377 0.04509151 0.04602828 0.0464417  0.04870854 0.04559755
 0.04538387]
Model epoch 72: train total loss -39.243760799994725, train mean loss 0.04196189286145558, test mean loss [0.04530736 0.04628867 0.04765318 0.0465007  0.04611718 0.04712448
 0.0455993 ]
Model epoch 73: train total loss -39.31178376921277, train mean loss 0.04206022873173445, test mean loss [0.04391189 0.04590663 0.04614802 0.04679399 0.04855679 0.04460832
 0.04566677]
Model epoch 74: train total loss -39.4203681229038, train mean loss 0.042223197330734985, test mean loss [0.04594373 0.04616118 0.04661879 0.04660516 0.04636183 0.04731379
 0.04485404]
Model epoch 75: train total loss -39.42204071448136, train mean loss 0.04137989698677185, test mean loss [0.04365795 0.04582513 0.04577734 0.04647906 0.04670039 0.04677179
 0.04472239]
Model epoch 76: train total loss -39.45954299072656, train mean loss 0.041365395880708034, test mean loss [0.04361197 0.04646228 0.04507413 0.04503202 0.04680085 0.04706763
 0.04462043]
Model epoch 77: train total loss -39.52826441658311, train mean loss 0.04229024175065966, test mean loss [0.0430179  0.04523516 0.04574727 0.04650178 0.04767231 0.04494934
 0.04405462]
Model epoch 78: train total loss -39.554139960618826, train mean loss 0.041016048273928234, test mean loss [0.04540341 0.04560019 0.04440517 0.04535306 0.04627187 0.04557607
 0.0442453 ]
Model epoch 79: train total loss -39.65626113280386, train mean loss 0.04091075748000108, test mean loss [0.04515356 0.0433056  0.04455452 0.04616602 0.0457412  0.04646544
 0.04489914]
Model epoch 80: train total loss -39.66332040018415, train mean loss 0.040808020116266226, test mean loss [0.04335832 0.04439831 0.04475211 0.04611768 0.0464761  0.04379879
 0.04383775]
Model epoch 81: train total loss -39.727446753780725, train mean loss 0.040412706817059586, test mean loss [0.04454215 0.04289693 0.04386635 0.04374887 0.04675239 0.04418378
 0.04491908]
Model epoch 82: train total loss -39.722863497548474, train mean loss 0.04091351641955107, test mean loss [0.0431421  0.04407737 0.04466957 0.04586762 0.04534179 0.044585
 0.04375834]
Model epoch 83: train total loss -39.82621416353243, train mean loss 0.04037602167982684, test mean loss [0.04420477 0.04299027 0.04421542 0.04460139 0.04877638 0.04280011
 0.04330339]
Model epoch 84: train total loss -39.93653787040944, train mean loss 0.04000477254946414, test mean loss [0.04562177 0.04303972 0.04395591 0.04495955 0.04389076 0.04383659
 0.0440831 ]
Model epoch 85: train total loss -39.88820263952774, train mean loss 0.040732200181834516, test mean loss [0.04130545 0.04350536 0.04629698 0.04457182 0.04568134 0.04274201
 0.0424346 ]
Model epoch 86: train total loss -40.05376182928679, train mean loss 0.04007699319303474, test mean loss [0.04264255 0.04476797 0.04510097 0.0433214  0.04391374 0.04563178
 0.04273106]
Model epoch 87: train total loss -40.02356519995988, train mean loss 0.03988719628351136, test mean loss [0.04303963 0.04393583 0.04259888 0.04493709 0.04534123 0.04314357
 0.04273071]
Model epoch 88: train total loss -40.078635409884185, train mean loss 0.039827742455079175, test mean loss [0.04253829 0.04168213 0.04305518 0.04363498 0.04481179 0.04346869
 0.04177711]
Model epoch 89: train total loss -40.11663103455323, train mean loss 0.03965451552755441, test mean loss [0.04164042 0.04560358 0.04331612 0.04300817 0.04517534 0.04423331
 0.04313211]
Model epoch 90: train total loss -40.21579387223948, train mean loss 0.03921258753135004, test mean loss [0.04160623 0.04215163 0.04258179 0.04402932 0.04511115 0.04233836
 0.04307843]
Model epoch 91: train total loss -40.239155765464204, train mean loss 0.03958749268865202, test mean loss [0.04272885 0.04330088 0.04265685 0.0443155  0.04416762 0.04449388
 0.04399618]
Model epoch 92: train total loss -40.1970272174519, train mean loss 0.03946490237238328, test mean loss [0.04181043 0.0441846  0.04325834 0.0435036  0.04423956 0.04350352
 0.04295937]
Model epoch 93: train total loss -40.232082655055635, train mean loss 0.03934972092483318, test mean loss [0.04184374 0.04218374 0.04374396 0.04415736 0.04314207 0.0430147
 0.04293371]
Model epoch 94: train total loss -40.290130302348445, train mean loss 0.039481890498851126, test mean loss [0.04249947 0.04156269 0.04191853 0.04357859 0.04413844 0.04217281
 0.04247474]
Model epoch 95: train total loss -40.37730591517932, train mean loss 0.03875592396998514, test mean loss [0.0418268  0.04221122 0.04276747 0.04310305 0.04440359 0.04256218
 0.0408144 ]
Model epoch 96: train total loss -40.51634252649762, train mean loss 0.03847283178292582, test mean loss [0.04069193 0.04330145 0.0428074  0.04320553 0.04217019 0.04317391
 0.0427244 ]
Model epoch 97: train total loss -40.46476568803529, train mean loss 0.038600142700347916, test mean loss [0.0423334  0.04186844 0.04229566 0.04408956 0.0440631  0.04142537
 0.04245827]
Model epoch 98: train total loss -40.53163552709405, train mean loss 0.03859569616678029, test mean loss [0.04307603 0.04065005 0.04222429 0.04223616 0.04287736 0.0431232
 0.04256835]
Model epoch 99: train total loss -40.561658626917236, train mean loss 0.038029298280676894, test mean loss [0.04152072 0.04407828 0.04197427 0.04217136 0.04359204 0.04201304
 0.04149761]
Model epoch 100: train total loss -40.59151246626678, train mean loss 0.038599263766192875, test mean loss [0.04051493 0.0417265  0.041106   0.0430486  0.0431152  0.04239975
 0.04265721]
Model epoch 101: train total loss -40.55875028830276, train mean loss 0.038434320713336086, test mean loss [0.04302062 0.04101957 0.04066995 0.04155854 0.0458421  0.04112942
 0.04256393]
Model epoch 102: train total loss -40.616371796632976, train mean loss 0.03866313258966626, test mean loss [0.0408177  0.04176033 0.04340365 0.04206574 0.04272603 0.04170844
 0.04119273]
Model epoch 103: train total loss -40.606597625405634, train mean loss 0.0383019745445098, test mean loss [0.04197754 0.04147985 0.0402445  0.0440303  0.04274453 0.04095455
 0.04271524]
Model epoch 104: train total loss -40.70355970075898, train mean loss 0.03760802559462474, test mean loss [0.04147882 0.04144636 0.04195712 0.04071384 0.04307316 0.04156567
 0.04358246]
Model epoch 105: train total loss -40.76294176814681, train mean loss 0.03820408451912965, test mean loss [0.04344812 0.04118778 0.0402112  0.04272618 0.04260913 0.04193321
 0.04288776]
Model epoch 106: train total loss -40.78168820237409, train mean loss 0.03782474201454087, test mean loss [0.04211441 0.04078896 0.04361861 0.04291124 0.04252554 0.04124631
 0.04266544]
Model epoch 107: train total loss -40.84485244833068, train mean loss 0.03698740592460578, test mean loss [0.04135696 0.04106667 0.03996663 0.04057114 0.04329465 0.04067472
 0.04201964]
Model epoch 108: train total loss -40.8467835996289, train mean loss 0.037718931283931195, test mean loss [0.04091347 0.04047468 0.04171153 0.04062965 0.04304655 0.0405843
 0.04219405]
Model epoch 109: train total loss -40.934326006143245, train mean loss 0.03737745967489206, test mean loss [0.04265906 0.03911853 0.04205302 0.04277417 0.04307189 0.04110058
 0.04188764]
Model epoch 110: train total loss -40.92485673343156, train mean loss 0.03762141861924263, test mean loss [0.04077583 0.04042188 0.04036011 0.04059874 0.04198729 0.04027653
 0.0426784 ]
Model epoch 111: train total loss -40.98673233100063, train mean loss 0.037121860681701, test mean loss [0.03926534 0.04105962 0.0405883  0.04119146 0.04266819 0.04040277
 0.04121881]
Model epoch 112: train total loss -40.9931450666538, train mean loss 0.037458918296157405, test mean loss [0.04184451 0.04147124 0.04138303 0.04364135 0.04229915 0.04089348
 0.04093052]
Model epoch 113: train total loss -40.957187456791566, train mean loss 0.037155268301071416, test mean loss [0.04116945 0.04152859 0.04061908 0.04142267 0.04272734 0.04114279
 0.04121808]
Model epoch 114: train total loss -41.04971385708825, train mean loss 0.036501091445309664, test mean loss [0.04112695 0.0400062  0.04076225 0.04058292 0.04181714 0.04156046
 0.0399488 ]
Model epoch 115: train total loss -41.07958431287833, train mean loss 0.036923576824181875, test mean loss [0.04171301 0.04093008 0.0396746  0.04105417 0.04259804 0.04132085
 0.0413398 ]
Model epoch 116: train total loss -41.110513666784655, train mean loss 0.03703236661793925, test mean loss [0.04104847 0.04055727 0.04025206 0.04230874 0.04283697 0.04040648
 0.04114196]
Model epoch 117: train total loss -41.11990994143289, train mean loss 0.03687361715743682, test mean loss [0.04286684 0.04008001 0.03954038 0.04004395 0.04274569 0.03938472
 0.04043909]
Model epoch 118: train total loss -41.09977866709958, train mean loss 0.037146712419741344, test mean loss [0.04100082 0.04201956 0.04168111 0.03992376 0.04079448 0.04506604
 0.04072504]
Model epoch 119: train total loss -41.28066698599595, train mean loss 0.036242784897319795, test mean loss [0.04154362 0.0413264  0.03917485 0.04005438 0.04301145 0.04290945
 0.04005326]
Model epoch 120: train total loss -41.17803119075236, train mean loss 0.036847189685337726, test mean loss [0.03940227 0.04019736 0.04023618 0.04156204 0.04121935 0.03994203
 0.04027601]
Model epoch 121: train total loss -41.24533433446159, train mean loss 0.0369908407812809, test mean loss [0.04072683 0.04070651 0.04084815 0.03993686 0.04101908 0.04040695
 0.04124663]
Model epoch 122: train total loss -41.26811456993157, train mean loss 0.03644080620488996, test mean loss [0.0423269  0.04069467 0.03933104 0.03992823 0.04146345 0.04047745
 0.04064139]
Model epoch 123: train total loss -41.3541762834444, train mean loss 0.0362300023514805, test mean loss [0.03951915 0.03973404 0.03897891 0.04111544 0.0408715  0.04015745
 0.04055035]
Model epoch 124: train total loss -41.43712259769785, train mean loss 0.0356819222854449, test mean loss [0.04077667 0.03833033 0.04018731 0.04044739 0.04222643 0.0391533
 0.04075917]
Model epoch 125: train total loss -41.456366542461076, train mean loss 0.035939443227268796, test mean loss [0.0404128  0.04062163 0.03995128 0.04097264 0.04257194 0.03931114
 0.03905804]
Model epoch 126: train total loss -41.41729923803726, train mean loss 0.036186443355591964, test mean loss [0.04060055 0.04153154 0.0385949  0.03997232 0.04051425 0.04061753
 0.04117124]
Model epoch 127: train total loss -41.526534230606316, train mean loss 0.03575592062125352, test mean loss [0.04139027 0.03921893 0.04117383 0.04065127 0.04041322 0.03879318
 0.03880858]
Model epoch 128: train total loss -41.55628751356554, train mean loss 0.03585840265585107, test mean loss [0.03991229 0.03914293 0.04043064 0.03979508 0.04012507 0.04026514
 0.03901332]
Model epoch 129: train total loss -41.57240213668249, train mean loss 0.036249053270636246, test mean loss [0.03979347 0.03935327 0.03919772 0.0395982  0.04084869 0.03846568
 0.04058169]
Model epoch 130: train total loss -41.59564230729382, train mean loss 0.03628711210628943, test mean loss [0.03938268 0.03954345 0.03864653 0.03928821 0.04194397 0.03975136
 0.03977463]
Model epoch 131: train total loss -41.614216529450275, train mean loss 0.03557493650548096, test mean loss [0.04007621 0.03994331 0.03959269 0.0396659  0.04155553 0.03926461
 0.04085758]
Model epoch 132: train total loss -41.60768900051067, train mean loss 0.03544318789007659, test mean loss [0.04102415 0.03959098 0.04072721 0.04064004 0.04078686 0.03864452
 0.04007607]
Model epoch 133: train total loss -41.67936774227327, train mean loss 0.03528175654066268, test mean loss [0.03965646 0.03800363 0.03887177 0.04020034 0.04127897 0.03918103
 0.03954937]
Model epoch 134: train total loss -41.72973298383863, train mean loss 0.03555425719913794, test mean loss [0.04010582 0.04046708 0.03838596 0.03911837 0.04155984 0.03996742
 0.04056914]
Model epoch 135: train total loss -41.74908728366747, train mean loss 0.03559655378122824, test mean loss [0.03820052 0.03941284 0.03882844 0.04051302 0.03935048 0.03878175
 0.0399485 ]
Model epoch 136: train total loss -41.771607784058965, train mean loss 0.035793089926585385, test mean loss [0.03992889 0.03990275 0.03879997 0.0416964  0.041558   0.03934975
 0.03855966]
Model epoch 137: train total loss -41.74708564466166, train mean loss 0.03492176940692884, test mean loss [0.03945429 0.03930463 0.03962211 0.03814396 0.0411994  0.03864042
 0.04071731]
Model epoch 138: train total loss -41.75065323552447, train mean loss 0.03521127003411809, test mean loss [0.04072738 0.03931505 0.0386722  0.03948567 0.03984275 0.03728622
 0.03914793]
Model epoch 139: train total loss -41.876901708379194, train mean loss 0.03547003662435271, test mean loss [0.03861507 0.03905916 0.03878561 0.03953071 0.04341567 0.03872201
 0.03913089]
Model epoch 140: train total loss -41.900395870836256, train mean loss 0.034577535658233106, test mean loss [0.03987031 0.03806349 0.03823907 0.03920587 0.04043358 0.03947689
 0.04075239]
Model epoch 141: train total loss -41.87772405530725, train mean loss 0.035735280732603125, test mean loss [0.03941228 0.03854895 0.04099994 0.03883691 0.04084329 0.03776753
 0.04046577]
Model epoch 142: train total loss -41.891862855293155, train mean loss 0.035200202453560626, test mean loss [0.03813545 0.0386744  0.03861064 0.03971026 0.04002981 0.03894725
 0.03906311]
Model epoch 143: train total loss -41.899448828558434, train mean loss 0.03482335179440111, test mean loss [0.0397134  0.03929817 0.03857317 0.03775571 0.03913023 0.03880033
 0.03824531]
Model epoch 144: train total loss -41.89279263090481, train mean loss 0.035000752982003495, test mean loss [0.03914325 0.0381041  0.03960774 0.0391218  0.03982818 0.03959601
 0.03840911]
Model epoch 145: train total loss -42.057621159322515, train mean loss 0.03370060094888455, test mean loss [0.04041148 0.03796411 0.03883185 0.03978907 0.03892672 0.03945539
 0.03993466]
Model epoch 146: train total loss -41.96696037974414, train mean loss 0.03412433024977029, test mean loss [0.0386633  0.03841863 0.03747847 0.03979672 0.04225007 0.03846028
 0.03881888]
Model epoch 147: train total loss -41.92386840589265, train mean loss 0.03515553254972617, test mean loss [0.03952284 0.03807637 0.03723993 0.03794892 0.04087408 0.03931419
 0.03886097]
Model epoch 148: train total loss -41.978486450541794, train mean loss 0.03472500945081571, test mean loss [0.03962002 0.03789926 0.03866343 0.03704926 0.03965591 0.03840098
 0.03982049]
Model epoch 149: train total loss -42.04478686358394, train mean loss 0.03434596984943176, test mean loss [0.04006588 0.03867383 0.03907008 0.04136952 0.03974329 0.03704872
 0.04046993]
Model epoch 150: train total loss -42.091349735060156, train mean loss 0.03444897278770005, test mean loss [0.03972802 0.03930617 0.03748405 0.03985615 0.03850515 0.03860877
 0.0390838 ]
Model epoch 151: train total loss -41.981676388496325, train mean loss 0.03445849770244552, test mean loss [0.03830742 0.03957636 0.03914096 0.03868081 0.04054614 0.03770445
 0.03855272]
Model epoch 152: train total loss -42.15987980816467, train mean loss 0.034396327116213134, test mean loss [0.03872926 0.03837856 0.0389615  0.0376807  0.03871593 0.03830734
 0.03856513]
Model epoch 153: train total loss -42.17775013766631, train mean loss 0.03487275482400063, test mean loss [0.03837216 0.03777343 0.03704869 0.0386218  0.03958439 0.03830578
 0.03863253]
Model epoch 154: train total loss -42.14823713809941, train mean loss 0.033886862948759394, test mean loss [0.03857004 0.03883989 0.03815058 0.03739163 0.03843906 0.03757267
 0.03880188]
Model epoch 155: train total loss -42.166828646867714, train mean loss 0.03369079359757943, test mean loss [0.03940596 0.03802658 0.03765911 0.03770221 0.03916167 0.038235
 0.03813576]
Model epoch 156: train total loss -42.28171041019786, train mean loss 0.0328299297664894, test mean loss [0.03809803 0.03763714 0.04088326 0.03945038 0.03907064 0.03829401
 0.03928796]
Model epoch 157: train total loss -42.302697518210394, train mean loss 0.03335688204455028, test mean loss [0.03833275 0.03846268 0.03822737 0.03891924 0.03883718 0.03662818
 0.03948526]
Model epoch 158: train total loss -42.317002483028475, train mean loss 0.034052795717238654, test mean loss [0.03982517 0.03829118 0.03866661 0.03721718 0.0401847  0.03688266
 0.03815168]
Model epoch 159: train total loss -42.34542929413534, train mean loss 0.03372206809698992, test mean loss [0.03917556 0.03774273 0.03700468 0.0373031  0.03758742 0.03877066
 0.03953434]
Model epoch 160: train total loss -42.45061854407055, train mean loss 0.032443213634516986, test mean loss [0.03883249 0.03706067 0.03933178 0.03834612 0.03762564 0.03745953
 0.03840682]
Model epoch 161: train total loss -42.25900883302541, train mean loss 0.033078650315346014, test mean loss [0.03857938 0.04025009 0.03746679 0.03841335 0.03848396 0.03779123
 0.03729364]
Model epoch 162: train total loss -42.34392243469086, train mean loss 0.03341506618421808, test mean loss [0.0384476  0.03889056 0.03668896 0.03813317 0.03955492 0.03706789
 0.04015038]
Model epoch 163: train total loss -42.275075151881616, train mean loss 0.03352912970894836, test mean loss [0.03767574 0.03681845 0.03900159 0.03794707 0.03828407 0.03800594
 0.03940819]
Model epoch 164: train total loss -42.38778184061591, train mean loss 0.03358766688617785, test mean loss [0.03806819 0.03795752 0.03778014 0.0378758  0.03906335 0.03719699
 0.03936564]
Model epoch 165: train total loss -42.389615134268794, train mean loss 0.03266193010041935, test mean loss [0.03944413 0.03629021 0.03684759 0.03666359 0.03798621 0.03730591
 0.03865199]
Model epoch 166: train total loss -42.53033647804303, train mean loss 0.03321068663293306, test mean loss [0.03761267 0.03652238 0.03940178 0.03795232 0.03893987 0.03747573
 0.03965317]
Model epoch 167: train total loss -42.540914518815356, train mean loss 0.03282355497480981, test mean loss [0.03908292 0.03801699 0.0375193  0.03752333 0.03847744 0.03608144
 0.03894115]
Model epoch 168: train total loss -42.6217609157843, train mean loss 0.03233633001182574, test mean loss [0.03954163 0.03690215 0.03635916 0.03855518 0.03911413 0.03702323
 0.03845054]
Model epoch 169: train total loss -42.65566931464597, train mean loss 0.03236992496331746, test mean loss [0.0373783  0.03778381 0.03731497 0.03577618 0.03852284 0.03803149
 0.03801919]
Model epoch 170: train total loss -42.55405171833261, train mean loss 0.03369511865922425, test mean loss [0.03772696 0.03734262 0.03788296 0.03757796 0.03819449 0.03745318
 0.0378491 ]
Model epoch 171: train total loss -42.62834280506663, train mean loss 0.032925059936968364, test mean loss [0.03717389 0.03747422 0.0372562  0.03761678 0.03779811 0.03700319
 0.03786906]
Model epoch 172: train total loss -42.72318674993493, train mean loss 0.03242667622769702, test mean loss [0.03781755 0.03672504 0.03677406 0.03750271 0.0372787  0.03647909
 0.03908454]
Model epoch 173: train total loss -42.68960365397395, train mean loss 0.03275827613420761, test mean loss [0.03836651 0.03637724 0.03770517 0.03594841 0.03864209 0.03878163
 0.03680728]
Model epoch 174: train total loss -42.7273216761337, train mean loss 0.032428974384510756, test mean loss [0.03711937 0.03721432 0.03652396 0.03736454 0.03899197 0.03810846
 0.0377125 ]
Model epoch 175: train total loss -42.8007970954022, train mean loss 0.03214436543426807, test mean loss [0.03701586 0.03743085 0.03753993 0.03679158 0.0386934  0.037394
 0.0393265 ]
Model epoch 176: train total loss -42.7585333804902, train mean loss 0.032584833938676665, test mean loss [0.03904612 0.03825922 0.03812933 0.03659008 0.03729235 0.03713436
 0.03798388]
Model epoch 177: train total loss -42.832248373166564, train mean loss 0.032448642872542836, test mean loss [0.0374957  0.03741068 0.0383801  0.03665615 0.03718674 0.03705513
 0.03700747]
Model epoch 178: train total loss -42.830703980596155, train mean loss 0.03238937357154059, test mean loss [0.0374355  0.03592972 0.03716483 0.03752592 0.03922602 0.03662028
 0.03678048]
Model epoch 179: train total loss -42.95740737115226, train mean loss 0.03189654012903099, test mean loss [0.03606813 0.03773525 0.03852809 0.03541089 0.03824496 0.03790856
 0.03789903]
Model epoch 180: train total loss -42.876844447629146, train mean loss 0.032070609407503205, test mean loss [0.03807251 0.03637976 0.03938593 0.03723661 0.03842052 0.03569891
 0.03827167]
Model epoch 181: train total loss -42.872576680261275, train mean loss 0.03213476984449315, test mean loss [0.03784356 0.03628205 0.03614234 0.03646744 0.03748014 0.038031
 0.03737602]
Model epoch 182: train total loss -42.926984692233276, train mean loss 0.03203184851385411, test mean loss [0.03741995 0.03692219 0.03627601 0.03651598 0.03691771 0.03680408
 0.03632972]
Model epoch 183: train total loss -43.03593586933996, train mean loss 0.03184361080150531, test mean loss [0.03673074 0.0368595  0.0356206  0.03664493 0.03843724 0.0361921
 0.03843729]
Model epoch 184: train total loss -43.0338300286194, train mean loss 0.032051243025142916, test mean loss [0.03550548 0.03611919 0.03690471 0.03650809 0.03863594 0.0369615
 0.03745953]
Model epoch 185: train total loss -43.015983364173245, train mean loss 0.03173680141841779, test mean loss [0.03712442 0.03692395 0.03720467 0.03757803 0.03705461 0.03629177
 0.03693039]
Model epoch 186: train total loss -42.994375337791936, train mean loss 0.031615692486255925, test mean loss [0.03782411 0.03533828 0.03737652 0.03570015 0.03816974 0.03698316
 0.03741644]
Model epoch 187: train total loss -43.04841547960785, train mean loss 0.03130051688592394, test mean loss [0.03707941 0.03646125 0.03647875 0.03609847 0.03779209 0.03690925
 0.03813519]
Model epoch 188: train total loss -43.051555729469, train mean loss 0.0317365742386929, test mean loss [0.03752349 0.03631888 0.03519996 0.03543293 0.03707117 0.03624046
 0.03838669]
Model epoch 189: train total loss -43.035285121858124, train mean loss 0.031431036781731085, test mean loss [0.03627433 0.03611212 0.03671026 0.03652459 0.03727292 0.03768704
 0.03736344]
Model epoch 190: train total loss -43.03382088135303, train mean loss 0.03196805012777607, test mean loss [0.03616305 0.03472177 0.03576285 0.03609969 0.03718575 0.03548742
 0.03982857]
Model epoch 191: train total loss -43.01540083456475, train mean loss 0.031196882628180433, test mean loss [0.03711709 0.03732229 0.03721819 0.03564573 0.03806514 0.03621806
 0.03617285]
Model epoch 192: train total loss -43.13449819709692, train mean loss 0.031149601397137714, test mean loss [0.03567526 0.03588667 0.03516227 0.03612615 0.03765964 0.03628649
 0.03543602]
Model epoch 193: train total loss -43.13574898381747, train mean loss 0.0308379854040381, test mean loss [0.03675227 0.03668601 0.03785373 0.0352792  0.03613756 0.03532744
 0.0378572 ]
Model epoch 194: train total loss -43.09280930330658, train mean loss 0.03154796920443807, test mean loss [0.03622851 0.0355411  0.0362873  0.03592869 0.03673509 0.03722131
 0.03682635]
Model epoch 195: train total loss -43.2334469345761, train mean loss 0.03111163231037388, test mean loss [0.03639215 0.03483779 0.03655455 0.03606913 0.03717177 0.03586211
 0.03698243]
Model epoch 196: train total loss -43.23814777399994, train mean loss 0.031046670921523814, test mean loss [0.03706524 0.0359739  0.03449647 0.0360677  0.03734124 0.03464767
 0.0362556 ]
Model epoch 197: train total loss -43.21763954792221, train mean loss 0.030777464790836706, test mean loss [0.03752434 0.03624468 0.03623937 0.03387751 0.03717347 0.03671603
 0.03667297]
Model epoch 198: train total loss -43.27119049417131, train mean loss 0.03099649872175015, test mean loss [0.03573201 0.03591115 0.03699674 0.03773195 0.03835023 0.03607913
 0.03612135]
Model epoch 199: train total loss -43.244126817024664, train mean loss 0.03128966910249582, test mean loss [0.03656828 0.03606736 0.03506573 0.03536206 0.03857092 0.03602776
 0.03748823]
Model epoch 200: train total loss -43.26926394006161, train mean loss 0.030879026779124463, test mean loss [0.03651465 0.03607903 0.03556493 0.03416113 0.03607651 0.03661014
 0.03619441]
Model epoch 201: train total loss -43.33870646025425, train mean loss 0.031305460199155394, test mean loss [0.03658753 0.03575898 0.03545774 0.03697863 0.03747779 0.03558127
 0.03640716]
Model epoch 202: train total loss -43.287429824647596, train mean loss 0.031387545336519634, test mean loss [0.03525278 0.03658556 0.03608363 0.03582223 0.03807105 0.03633454
 0.03779848]
Model epoch 203: train total loss -43.34790752722565, train mean loss 0.031010786003813053, test mean loss [0.03658494 0.03609528 0.03640277 0.0351163  0.03680737 0.0356698
 0.03631802]
Model trained in 204 epochs with 1000 transitions.
/workspace/submodules/ssrl/ssrl/brax/training/replay_buffers.py:121: FutureWarning:
jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.
{'steps': 1000, 'epoch': 0, 'training/model_train_time': 197.77710843086243, 'training/other_time': 14.517459392547607, 'training/model_horizon': 1, 'training/hallucination_updates_per_training_step': 10, 'training/env_buffer_size': Array(1000, dtype=int32), 'training/reset_critic': 0, 'model/train_total_loss': Array(-43.34790753, dtype=float64, weak_type=True), 'model/train_mean_loss': Array(0.03101079, dtype=float64), 'model/test_total_loss': Array(-40.95223065, dtype=float64), 'model/test_mean_loss': Array(0.03614207, dtype=float64), 'model/train_epochs': 204, 'model/sec_per_epoch': 0.9530231765672272, 'sac/actor_loss': Array(4.6924902, dtype=float64), 'sac/alpha': Array(0.9542946, dtype=float32), 'sac/alpha_loss': Array(2.025564, dtype=float64), 'sac/buffer_current_size': Array(3020., dtype=float32), 'sac/critic_loss': Array(2.41565889, dtype=float64), 'rollout/stats/avg_episode_length': Array(1005., dtype=float64), 'rollout/stats/max_episode_length': Array(1005., dtype=float64), 'rollout/stats/avg_forward_vel': Array(0.01364337, dtype=float64), 'rollout/stats/first_avg_forward_vel': Array(0.01364337, dtype=float64), 'rollout/stats/avg_side_vel': Array(-0.02647018, dtype=float64), 'rollout/rollout_steps': 1000, 'rollout/stats/avg_reward_per_step': Array(0.25248575, dtype=float64), 'rollout/reward': Array(252.48574798, dtype=float64), 'rollout/total_reward': Array(252.48574798, dtype=float64), 'rollout/rew/penalty_torque_lim': Array(-0.09928996, dtype=float64), 'rollout/rew/rew_action': Array(0., dtype=float64), 'rollout/rew/rew_ang_change': Array(0.05124828, dtype=float64), 'rollout/rew/rew_ang_vel': Array(0., dtype=float64), 'rollout/rew/rew_cosmetic': Array(0., dtype=float64), 'rollout/rew/rew_energy': Array(0.00057291, dtype=float64), 'rollout/rew/rew_foot_z': Array(0., dtype=float64), 'rollout/rew/rew_forward_vel': Array(0.00586812, dtype=float64), 'rollout/rew/rew_joint_acc': Array(0., dtype=float64), 'rollout/rew/rew_joint_limits': Array(0., dtype=float64), 'rollout/rew/rew_pitch': Array(0.04978685, dtype=float64), 'rollout/rew/rew_roll': Array(0.0535181, dtype=float64), 'rollout/rew/rew_side_motion': Array(0.10024919, dtype=float64), 'rollout/rew/rew_torque_limits': Array(0., dtype=float64), 'rollout/rew/rew_turn': Array(0.03633915, dtype=float64), 'rollout/rew/rew_yaw': Array(0.02344678, dtype=float64), 'rollout/rew/rew_z_vel_change': Array(0.03225805, dtype=float64), 'rollout/stats/avg_turn_rate': Array(-0.11795346, dtype=float64)}